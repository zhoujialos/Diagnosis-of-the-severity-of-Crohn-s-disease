import os
os.environ.setdefault('CUBLAS_WORKSPACE_CONFIG', ':16:8')
from typing import Dict, Tuple
import warnings
warnings.filterwarnings('ignore')
from PerClassInspector import PerClassInspector
# æ•°æ®å¤„ç†ç›¸å…³
import numpy as np
import pandas as pd
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.metrics import f1_score, accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
import xgboost as xgb
import lightgbm as lgb
import torch
torch.use_deterministic_algorithms(True)
import torch.nn as nn
import torch.nn.functional as F
from pytorch_metric_learning import losses, miners

# è´å¶æ–¯ä¼˜åŒ–
import optuna
from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler
import numpy as np
# å¯è§†åŒ–ç›¸å…³
import matplotlib.pyplot as plt
import seaborn as sns
from visualize_metric_learning import MetricLearningVisualizer
# ä¿®å¤numpyç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
np.int = int
def set_all_seeds(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§"""
    import random
    import os
    
    # PythonåŸç”Ÿéšæœº
    random.seed(seed)
    
    # NumPyéšæœº
    np.random.seed(seed)
    
    # PyTorchéšæœº
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # CuDNNç¡®å®šæ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # ç¯å¢ƒå˜é‡è®¾ç½®
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    print(f"å·²è®¾ç½®æ‰€æœ‰éšæœºç§å­ä¸º: {seed}")

class ComparisonConfig:
    """ä¸‰ç§æ–¹æ³•å¯¹æ¯”é…ç½®"""
    
    # æ•°æ®è·¯å¾„
    TRAIN_DATA_PATH = 'train_shap.csv'
    TEST_DATA_PATH = 'test_shap.csv'
    
    # åŸºç¡€é…ç½®
    N_SPLITS = 5
    RANDOM_STATE = 42
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # ä¼˜åŒ–ç›¸å…³
    N_TRIALS = 100  
    N_WARMUP_STEPS = 10
    N_STARTUP_TRIALS = 5
    
    # Few-Shot Learningé…ç½®
    N_WAY = 3  # 3ç±»åˆ†ç±»
    N_SUPPORT = 5  # æ¯ç±»5ä¸ªæ”¯æŒæ ·æœ¬
    N_QUERY = 3   # æ¯ç±»3ä¸ªæŸ¥è¯¢æ ·æœ¬
    
    # æ–‡æœ¬ç‰¹å¾è¯æ±‡è¡¨
    VOCABULARY = {
        "thickening and narrowing of the small intestine": 0,
        "thickening and narrowing of the colon": 1,
        "rectal thickening and narrowing": 2,
        "thickening and narrowing of the small intestine with expansion": 3,
        "small intestinal fistula": 4,
        "colon fistula": 5,
        "anal fistula": 6,
        "small bowel abscess": 7,
        "rectal fistula": 8,
        "thickening of the small intestine": 9,
        "thickening and narrowing of the colon with expansion": 10,
        "anal abscess": 11,
        "colon abscess": 12
    }
    
    # ç‰¹å¾åˆ—å
    CONTINUOUS_FEATURES = ['CRP', 'age']
    CATEGORICAL_FEATURES = ['CDAI_score', 'SESCD_score', 'FC', 'gender', 'smoking', 'education']
    
    @classmethod
    def get_cv_splitter(cls):
        """è·å–ç»Ÿä¸€çš„äº¤å‰éªŒè¯åˆ†å‰²å™¨"""
        return StratifiedKFold(
            n_splits=cls.N_SPLITS,
            shuffle=True,
            random_state=cls.RANDOM_STATE
        )

class DataProcessor:
    """æ•°æ®é¢„å¤„ç†ç±»ï¼ˆæ–‡æœ¬çŸ­è¯­ -> äºŒå€¼ 0/1ï¼‰"""

    def __init__(self, vocabulary: Dict[str, int]):
        self.vocabulary = vocabulary
        # å‡ºç°=1ï¼Œä¸å‡ºç°=0
        self.vectorizer = CountVectorizer(
            vocabulary=vocabulary,     # é”®=çŸ­è¯­ï¼Œå€¼=åˆ—ç´¢å¼•
            ngram_range=(1, 7),        # è¦†ç›– multi-word çŸ­è¯­
            binary=True,               # å…³é”®ï¼šäºŒå€¼
            lowercase=True,            # ä¸æ¸…æ´—ä¸€è‡´
            stop_words=None            
        )
        self.scaler = StandardScaler()
        self.is_fitted = False
        self.ordinal_features = ['CDAI_score', 'SESCD_score', 'FC']
        self.nominal_features = ['gender', 'smoking', 'education']
        self.nominal_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')

    @staticmethod
    def replace_text(text: str) -> str:
        """å°å†™ã€å»æ ‡ç‚¹ï¼ˆä¿ç•™ç©ºæ ¼ï¼‰ï¼Œä¸åˆ é™¤ and"""
        if pd.isna(text):
            return ""
        s = str(text).lower()
        s = re.sub(r'[^a-z0-9\s]', ' ', s)   # æ ‡ç‚¹ -> ç©ºæ ¼
        s = re.sub(r'\s+', ' ', s).strip()
        return s

    def fill_missing_values(self, data: pd.DataFrame) -> pd.DataFrame:
        data = data.copy()
        for col in ComparisonConfig.CONTINUOUS_FEATURES + ComparisonConfig.CATEGORICAL_FEATURES:
            if col in data.columns:
                median_val = data[col].median()
                data[col].fillna(median_val, inplace=True)
        return data

    def extract_features(self, data: pd.DataFrame, fit_transform: bool = False) -> np.ndarray:
        """æå–æ‰€æœ‰ç‰¹å¾ï¼šæ–‡æœ¬çŸ­è¯­(0/1) + åˆ†ç±»åŸå€¼ + è¿ç»­(æ ‡å‡†åŒ–)"""
        data = data.copy()
        data["processed_text"] = data["text"].apply(self.replace_text)
        data = self.fill_missing_values(data)

        # æ–‡æœ¬ç‰¹å¾ -> 0/1
        if fit_transform:
            text_features = self.vectorizer.fit_transform(data['processed_text'].values.astype('U'))
        else:
            if not self.is_fitted:
                self.vectorizer.fit(data['processed_text'].values.astype('U'))
            text_features = self.vectorizer.transform(data['processed_text'].values.astype('U'))

        # è¿ç»­ç‰¹å¾ï¼ˆæ ‡å‡†åŒ–ï¼‰
        continuous_features = data[ComparisonConfig.CONTINUOUS_FEATURES].values.astype(np.float64)
        if fit_transform:
            continuous_features = self.scaler.fit_transform(continuous_features)
            self.is_fitted = True
        else:
            continuous_features = self.scaler.transform(continuous_features)

        categorical_features = data[ComparisonConfig.CATEGORICAL_FEATURES].values.astype(np.float64)
        # æ‹¼æ¥
        all_features = np.concatenate([
            text_features.toarray().astype(np.float32),   # 0/1
            categorical_features.astype(np.float32),
            continuous_features.astype(np.float32)
        ], axis=1)

        return all_features

    # ç»™å‡ºä¸æ‹¼æ¥é¡ºåºä¸€è‡´çš„åˆ—åï¼Œæ–¹ä¾¿ SHAP
    def get_feature_names(self) -> list:
        text_names = [None] * len(self.vocabulary)
        for k, i in self.vocabulary.items():
            text_names[i] = k
        return text_names + ComparisonConfig.CATEGORICAL_FEATURES + ComparisonConfig.CONTINUOUS_FEATURES


class MedicalDataset(torch.utils.data.Dataset):
    """åŒ»ç–—æ•°æ®é›†ç±»"""
    
    def __init__(self, features: np.ndarray, labels: np.ndarray, 
                 transform=None, device: str = 'cpu'):
        self.device = device
        self.transform = transform
        
        if hasattr(features, 'values'):
            features = features.values
        if hasattr(labels, 'values'):
            labels = labels.values
        
        self.features = torch.from_numpy(features).float()
        self.labels = torch.from_numpy(labels).long()
        
        if self.device != 'cpu':
            self.features = self.features.to(self.device)
            self.labels = self.labels.to(self.device)
    
    def __len__(self) -> int:
        return len(self.features)
    
    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor]:
        features = self.features[index]
        label = self.labels[index]
        
        if self.transform is not None:
            features = self.transform(features)
            
        return features, label

class FewShotDataset(torch.utils.data.Dataset):
    """Few-Shot Learningæ•°æ®é›†"""
    
    def __init__(self, features: np.ndarray, labels: np.ndarray, 
                 n_way: int, n_support: int, n_query: int, device: str = 'cpu'):
        self.device = device
        self.n_way = n_way
        self.n_support = n_support
        self.n_query = n_query
        
        # è½¬æ¢ä¸ºtensor
        if hasattr(features, 'values'):
            features = features.values
        if hasattr(labels, 'values'):
            labels = labels.values
            
        self.features = torch.from_numpy(features).float()
        self.labels = torch.from_numpy(labels).long()
        
        # æŒ‰ç±»åˆ«ç»„ç»‡æ•°æ®
        self.classes = torch.unique(self.labels).tolist()
        self.class_to_indices = {}
        for class_id in self.classes:
            self.class_to_indices[class_id] = torch.where(self.labels == class_id)[0]
        
        if self.device != 'cpu':
            self.features = self.features.to(self.device)
            self.labels = self.labels.to(self.device)
    
    def __len__(self) -> int:
        return 1000  # ç”Ÿæˆ1000ä¸ªepisode
    
    def __getitem__(self, index: int):
        """ç”Ÿæˆä¸€ä¸ªFew-Shot Learning episode"""
        # éšæœºé€‰æ‹©n_wayä¸ªç±»åˆ«
        selected_classes = np.random.choice(self.classes, self.n_way, replace=False)
        
        support_features = []
        support_labels = []
        query_features = []
        query_labels = []
        
        for i, class_id in enumerate(selected_classes):
            class_indices = self.class_to_indices[class_id]
            need = self.n_support + self.n_query
            if len(class_indices) >= need:
                selected = class_indices[torch.randperm(len(class_indices), device=class_indices.device)[:need]]
            else:
                sel_idx = torch.randint(0, len(class_indices), (need,), device=class_indices.device)
                selected = class_indices[sel_idx]

            support_indices = selected[:self.n_support]
            query_indices = selected[self.n_support:]

            support_features.append(self.features[support_indices])
            query_features.append(self.features[query_indices])

            support_labels.extend([i] * support_indices.numel())
            query_labels.extend([i] * query_indices.numel())
# è½¬æ¢ä¸ºtensor
        support_features = torch.cat(support_features, dim=0)
        query_features = torch.cat(query_features, dim=0)
        support_labels = torch.tensor(support_labels, dtype=torch.long)
        query_labels = torch.tensor(query_labels, dtype=torch.long)
        
        if self.device != 'cpu':
            support_labels = support_labels.to(self.device)
            query_labels = query_labels.to(self.device)
        
        return support_features, support_labels, query_features, query_labels

# ==================== å…±äº«ç‰¹å¾æå–å™¨ ====================
class SharedBackbone(nn.Module):
    """ä¸‰ç§æ–¹æ³•å…±äº«çš„ç‰¹å¾æå–å™¨"""
    
    def __init__(self, input_size: int, hidden_size: int, embedding_size: int, 
                 dropout_rate: float, device: str = 'cpu'):
        super().__init__()
        
        # Trunkç½‘ç»œ
        self.trunk = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.7)
        )
        
        # Embedderç½‘ç»œ
        self.embedder = nn.Sequential(
            nn.Linear(hidden_size // 2, embedding_size),
            nn.ReLU()
        )
        
        self.to(device)
    
    def forward(self, x):
        trunk_output = self.trunk(x)
        embeddings = self.embedder(trunk_output)
        return trunk_output, embeddings

# ==================== 1. åº¦é‡å­¦ä¹ æ–¹æ³• ====================
class MetricLearningModel(nn.Module):
    """åº¦é‡å­¦ä¹ æ¨¡å‹"""
    
    def __init__(self, input_size: int, hidden_size: int, embedding_size: int, 
                 num_classes: int, dropout_rate: float, device: str = 'cpu'):
        super().__init__()
        
        # å…±äº«backbone
        self.backbone = SharedBackbone(input_size, hidden_size, embedding_size, dropout_rate, device)
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Linear(embedding_size, num_classes)
        
        self.to(device)
    
    def forward(self, x):
        trunk_output, embeddings = self.backbone(x)
        logits = self.classifier(embeddings)
        return trunk_output, embeddings, logits

# ==================== 2. åŒ¹é…ç½‘ç»œ ====================
class DistanceNetwork(nn.Module):
    """è·ç¦»ç½‘ç»œ"""
    
    def __init__(self):
        super(DistanceNetwork, self).__init__()
    
    def forward(self, support_embeddings, query_embeddings):
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        # å½’ä¸€åŒ–
        support_norm = F.normalize(support_embeddings, p=2, dim=1)
        query_norm = F.normalize(query_embeddings, p=2, dim=1)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ [n_query, n_support]
        similarities = torch.mm(query_norm, support_norm.t())
        return similarities


class AttentionalClassify(nn.Module):
    """æ³¨æ„åŠ›åˆ†ç±»å™¨"""

    def __init__(self):
        super(AttentionalClassify, self).__init__()

        # å¯å­¦ä¹ çš„æ¸©åº¦/ç¼©æ”¾ç³»æ•°ï¼ˆåˆå§‹åŒ–ä¸º 10.0ï¼Œå¸¸è§äºå¯¹æ¯”å­¦ä¹ /ArcFace ç­‰ï¼‰
        self.logit_scale = nn.Parameter(torch.tensor(10.0))
    def forward(self, similarities, support_labels):
        """
        Args:
            similarities: [n_query, n_support]  # ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæœªå½’ä¸€åŒ– logits
            support_labels: [n_support] (one-hotæˆ–æ ‡é‡)
        Returns:
            logits: [n_query, n_classes]
        """
        # å¯¹ç›¸ä¼¼åº¦æ–½åŠ å¯å­¦ä¹ ç¼©æ”¾ï¼Œç­‰ä»·äºæ¸©åº¦ç¼©æ”¾ï¼ˆæ¸©åº¦=1/scaleï¼‰
        similarities = similarities * self.logit_scale
        # ä¸å†å¯¹ similarities åš softmaxï¼Œç›´æ¥æŠŠæ”¯æŒé›†çš„æ ‡ç­¾ one-hot èšåˆä¸ºç±»æ‰“åˆ†
        if support_labels.dim() == 1:
            n_classes = support_labels.max().item() + 1
            support_labels_onehot = F.one_hot(support_labels, n_classes).float()
        else:
            support_labels_onehot = support_labels.float()

        logits = torch.mm(similarities, support_labels_onehot)  # [n_query, n_classes]
        return logits

class BidirectionalLSTM(nn.Module):
    """åŒå‘LSTMç”¨äºå…¨ä¸Šä¸‹æ–‡åµŒå…¥"""
    
    def __init__(self, embedding_size, hidden_size, device='cpu'):
        super(BidirectionalLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.device = device

        self.lstm = nn.LSTM(
            input_size=embedding_size,
            hidden_size=hidden_size,
            num_layers=1,
            bidirectional=True,
            batch_first=True
        )

    def forward(self, inputs, batch_size):
        """å‰å‘ä¼ æ’­"""
        h0 = torch.zeros(2, batch_size, self.hidden_size).to(self.device)
        c0 = torch.zeros(2, batch_size, self.hidden_size).to(self.device)
        
        if inputs.dim() == 2:
            inputs = inputs.unsqueeze(0)
        
        output, (hn, cn) = self.lstm(inputs, (h0, c0))
        return output.squeeze(0), hn, cn

class MatchingNetwork(nn.Module):
    """åŒ¹é…ç½‘ç»œ"""
    
    def __init__(self, input_size: int, hidden_size: int, embedding_size: int, 
                 num_classes: int, dropout_rate: float, use_fce: bool = False, 
                 lstm_hidden_size: int = 32, device: str = 'cpu'):
        super(MatchingNetwork, self).__init__()
        
        # å…±äº«backboneï¼ˆä¸åº¦é‡å­¦ä¹ å®Œå…¨ç›¸åŒï¼‰
        self.backbone = SharedBackbone(input_size, hidden_size, embedding_size, dropout_rate, device)
        
        self.use_fce = use_fce
        self.device = device
        
        # å¯é€‰çš„å…¨ä¸Šä¸‹æ–‡åµŒå…¥
        if use_fce:
            self.lstm = BidirectionalLSTM(embedding_size, lstm_hidden_size, device)
        
        # åŒ¹é…ç½‘ç»œç‰¹æœ‰ç»„ä»¶
        self.distance_network = DistanceNetwork()
        self.attentional_classify = AttentionalClassify()
        
        self.to(device)
    
    def forward(self, support_features, support_labels, query_features, query_labels=None):
        """
        Args:
            support_features: [n_support, feature_dim]
            support_labels: [n_support]
            query_features: [n_query, feature_dim] 
            query_labels: [n_query]
        """
        # ç‰¹å¾æå–ï¼ˆä¸åº¦é‡å­¦ä¹ ç›¸åŒï¼‰
        _, support_embeddings = self.backbone(support_features)
        _, query_embeddings = self.backbone(query_features)
        
        # å¯é€‰ï¼šå…¨ä¸Šä¸‹æ–‡åµŒå…¥
        if self.use_fce:
            # å¤„ç†æ”¯æŒé›†
            support_embeddings, _, _ = self.lstm(support_embeddings, support_embeddings.size(0))
            # å¤„ç†æŸ¥è¯¢é›†
            query_embeddings, _, _ = self.lstm(query_embeddings, query_embeddings.size(0))
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = self.distance_network(support_embeddings, query_embeddings)
        
        # æ³¨æ„åŠ›åˆ†ç±»
        predictions = self.attentional_classify(similarities, support_labels)
        
        return predictions

# ==================== 3. åŸå‹ç½‘ç»œ ====================
def euclidean_dist(x, y):
    """è®¡ç®—æ¬§æ°è·ç¦»"""
    # x: N x D, y: M x D
    n = x.size(0)
    m = y.size(0)
    d = x.size(1)
    if d != y.size(1):
        raise Exception("ç»´åº¦ä¸åŒ¹é…")

    x = x.unsqueeze(1).expand(n, m, d)
    y = y.unsqueeze(0).expand(n, m, d)

    return torch.pow(x - y, 2).sum(2)

class PrototypicalNetwork(nn.Module):
    """åŸå‹ç½‘ç»œ"""
    
    def __init__(self, input_size: int, hidden_size: int, embedding_size: int, 
                 num_classes: int, dropout_rate: float, device: str = 'cpu'):
        super(PrototypicalNetwork, self).__init__()
        
        # å…±äº«backboneï¼ˆä¸åº¦é‡å­¦ä¹ å®Œå…¨ç›¸åŒï¼‰
        self.backbone = SharedBackbone(input_size, hidden_size, embedding_size, dropout_rate, device)
        
        self.device = device
        self.to(device)
    
    def forward(self, support_features, support_labels, query_features, query_labels=None):
        """
        Args:
            support_features: [n_support, feature_dim]
            support_labels: [n_support]
            query_features: [n_query, feature_dim]
            query_labels: [n_query] (å¯é€‰ï¼Œç”¨äºè®­ç»ƒ)
        """
        # ç‰¹å¾æå–ï¼ˆä¸åº¦é‡å­¦ä¹ ç›¸åŒï¼‰
        _, support_embeddings = self.backbone(support_features)
        _, query_embeddings = self.backbone(query_features)
        
        # è®¡ç®—ç±»åŸå‹
        classes = torch.unique(support_labels)
        prototypes = []
        
        for class_id in classes:
            class_mask = support_labels == class_id
            class_embeddings = support_embeddings[class_mask]
            prototype = class_embeddings.mean(dim=0)  # ç±»ä¸­å¿ƒ
            prototypes.append(prototype)
        
        prototypes = torch.stack(prototypes)  # [n_classes, embedding_dim]
        
        # è®¡ç®—æŸ¥è¯¢æ ·æœ¬åˆ°åŸå‹çš„è·ç¦»
        distances = euclidean_dist(query_embeddings, prototypes)  # [n_query, n_classes]
        # ç›´æ¥è¿”å›ç±» logitsï¼ˆè´Ÿè·ç¦»ï¼‰ï¼Œäº¤ç»™å¤–éƒ¨çš„ F.cross_entropy å¤„ç†
        logits = -distances
        return logits

# ==================== ä¼ ç»Ÿæœºå™¨å­¦ä¹ è®­ç»ƒå™¨ ====================
class TraditionalMLTrainer:
    """ä¼ ç»Ÿæœºå™¨å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, train_features, train_labels, test_features, test_labels, train_df=None, processor=None):
        self.train_features = train_features
        self.train_labels = train_labels
        self.test_features = test_features
        self.test_labels = test_labels
        # ä¿ç•™åŸå§‹è®­ç»ƒDataFrameä¸å¤„ç†å™¨
        self.train_df = train_df
        self.base_processor = processor
        
        self.input_size = train_features.shape[1]
        self.num_classes = len(np.unique(train_labels))
        
        print(f"åˆå§‹åŒ–ä¼ ç»Ÿæœºå™¨å­¦ä¹ è®­ç»ƒå™¨:")
        print(f"  è¾“å…¥ç»´åº¦: {self.input_size}")
        print(f"  ç±»åˆ«æ•°: {self.num_classes}")
        print(f"  äº¤å‰éªŒè¯: {ComparisonConfig.N_SPLITS}æŠ˜") 
    
    def create_model_pipeline(self, model_name: str, trial):
        """åˆ›å»ºæ¨¡å‹ç®¡é“"""
        if model_name == 'svm':
            C = trial.suggest_float('C', 0.1, 10.0, log=True)
            kernel = trial.suggest_categorical('kernel', ['linear', 'rbf'])
            gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])
            
            return svm.SVC(
                C=C, kernel=kernel, gamma=gamma,
                random_state=ComparisonConfig.RANDOM_STATE,
                probability=True,  # ä¿®å¤AUCè¯„ä¼°é—®é¢˜
                class_weight='balanced'  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
            )
        
        elif model_name == 'random_forest':
            n_estimators = trial.suggest_int('n_estimators', 10, 200)
            max_depth = trial.suggest_categorical('max_depth', [None, 10, 50, 100])
            min_samples_split = trial.suggest_int('min_samples_split', 2, 10)
            criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])
            
            return RandomForestClassifier(
                n_estimators=n_estimators,
                max_depth=max_depth,
                min_samples_split=min_samples_split,
                criterion=criterion,
                random_state=ComparisonConfig.RANDOM_STATE,
                class_weight='balanced'  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
            )
        
        elif model_name == 'decision_tree':
            max_depth = trial.suggest_categorical('max_depth', [None, 10, 50, 100])
            min_samples_split = trial.suggest_int('min_samples_split', 2, 10)
            criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])
            
            return DecisionTreeClassifier(
                max_depth=max_depth,
                min_samples_split=min_samples_split,
                criterion=criterion,
                random_state=ComparisonConfig.RANDOM_STATE,
                class_weight='balanced'
            )
        
        elif model_name == 'xgboost':
            n_estimators = trial.suggest_int('n_estimators', 100, 300)
            max_depth = trial.suggest_int('max_depth', 3, 9)
            learning_rate = trial.suggest_float('learning_rate', 0.01, 0.2, log=True)
            
            return xgb.XGBClassifier(
                n_estimators=n_estimators,
                max_depth=max_depth,
                learning_rate=learning_rate,
                random_state=ComparisonConfig.RANDOM_STATE,
                eval_metric='mlogloss',
                class_weight='balanced'
            )
        
        elif model_name == 'lightgbm':
            num_leaves = trial.suggest_int('num_leaves', 31, 100)
            max_depth = trial.suggest_int('max_depth', 3, 9)
            learning_rate = trial.suggest_float('learning_rate', 0.01, 0.2, log=True)
            
            return lgb.LGBMClassifier(
                num_leaves=num_leaves,
                max_depth=max_depth,
                learning_rate=learning_rate,
                random_state=ComparisonConfig.RANDOM_STATE,
                objective='multiclass',
                num_class=self.num_classes,
                verbose=-1,
                class_weight='balanced'
            )
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_name}")
    
    def evaluate_sample_wise(self, model, val_features, val_labels):
        """ç»Ÿä¸€çš„é€æ ·æœ¬è¯„ä¼°æ–¹æ³•"""
        model.fit(self.train_features, self.train_labels)  # åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒ
        predictions = model.predict(val_features)
        accuracy = (predictions == val_labels).mean()
        return accuracy
    
    def objective(self, trial, model_name: str):
        """ä¼˜åŒ–ç›®æ ‡å‡½æ•° - ä½¿ç”¨ç»Ÿä¸€çš„äº¤å‰éªŒè¯"""
        # ä½¿ç”¨å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
        set_all_seeds(ComparisonConfig.RANDOM_STATE)
        
        try:
            cv_scores = []
            cv_f1_scores = []  # æ·»åŠ F1åˆ†æ•°åˆ—è¡¨
            # ä½¿ç”¨ç»Ÿä¸€çš„äº¤å‰éªŒè¯åˆ†å‰²å™¨
            skf = ComparisonConfig.get_cv_splitter()
            
            print(f"\nå¼€å§‹ {ComparisonConfig.N_SPLITS} æŠ˜äº¤å‰éªŒè¯...")
            
            for fold, (train_idx, val_idx) in enumerate(skf.split(self.train_features, self.train_labels)):
                print(f"  æ­£åœ¨å¤„ç†ç¬¬ {fold + 1}/{ComparisonConfig.N_SPLITS} æŠ˜...")
                
                # æŒ‰æŠ˜æ— æ³„æ¼ç‰¹å¾æå–ï¼šè‹¥æä¾›åŸå§‹train_dfä¸processorï¼Œåˆ™æ¯æŠ˜ç‹¬ç«‹fit/transform
                if self.train_df is not None:
                    # æ¯æŠ˜ä½¿ç”¨å…¨æ–°çš„ DataProcessorï¼ˆå…±äº«ç›¸åŒè¯è¡¨ï¼‰
                    vocab = self.base_processor.vocabulary if self.base_processor is not None else ComparisonConfig.VOCABULARY
                    fold_processor = DataProcessor(vocabulary=vocab)

                    fold_train_df = self.train_df.iloc[train_idx]
                    fold_val_df = self.train_df.iloc[val_idx]

                    fold_train_features = fold_processor.extract_features(fold_train_df, fit_transform=True)
                    fold_train_labels = fold_train_df["label"].to_numpy().astype(np.int64)

                    fold_val_features = fold_processor.extract_features(fold_val_df, fit_transform=False)
                    fold_val_labels = fold_val_df["label"].to_numpy().astype(np.int64)
                
                # åˆ›å»ºæ¨¡å‹
                model = self.create_model_pipeline(model_name, trial)
                
                # è®­ç»ƒå’Œè¯„ä¼°
                model.fit(fold_train_features, fold_train_labels)
                predictions = model.predict(fold_val_features)
                fold_score = (predictions == fold_val_labels).mean()
                fold_f1 = f1_score(fold_val_labels, predictions, average='weighted')  # æ·»åŠ F1åˆ†æ•°è®¡ç®—
                
                cv_scores.append(fold_score)
                cv_f1_scores.append(fold_f1)  # ä¿å­˜F1åˆ†æ•°
                print(f"    ç¬¬ {fold + 1} æŠ˜å‡†ç¡®ç‡: {fold_score:.4f}, F1: {fold_f1:.4f}")
                
                # å‰ªæåˆ¤æ–­
                trial.report(fold_score, fold)
                if trial.should_prune():
                    print(f"    è¯•éªŒåœ¨ç¬¬ {fold + 1} æŠ˜è¢«å‰ªæ")
                    raise optuna.TrialPruned()
            
            # è®¡ç®—å¹³å‡åˆ†æ•°
            avg_score = np.mean(cv_scores)
            std_score = np.std(cv_scores)
            avg_f1 = np.mean(cv_f1_scores)  # è®¡ç®—å¹³å‡F1åˆ†æ•°
            std_f1 = np.std(cv_f1_scores)   # è®¡ç®—F1åˆ†æ•°æ ‡å‡†å·®
            
            trial.set_user_attr('fold_scores', cv_scores)
            trial.set_user_attr('fold_f1_scores', cv_f1_scores)  # ä¿å­˜F1åˆ†æ•°
            print(f"  äº¤å‰éªŒè¯ç»“æœ: å‡†ç¡®ç‡ {avg_score:.4f} Â± {std_score:.4f}, F1 {avg_f1:.4f} Â± {std_f1:.4f}")
            
            return avg_score
            
        except optuna.TrialPruned:
            raise
        except Exception as e:
            print(f"è¯•éªŒå¤±è´¥: {e}")
            return 0.0
    
    def optimize(self, model_name: str):
        """æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–"""
        print(f"å¼€å§‹ä¼˜åŒ–ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹: {model_name}...")
        
        # åˆ›å»ºç ”ç©¶
        study = optuna.create_study(
            direction='maximize',
            pruner=MedianPruner(n_startup_trials=ComparisonConfig.N_STARTUP_TRIALS),
            sampler=TPESampler(seed=ComparisonConfig.RANDOM_STATE),
            study_name=f'traditional_ml_{model_name}_optimization'
        )
        
        # æ‰§è¡Œä¼˜åŒ–
        study.optimize(
            lambda trial: self.objective(trial, model_name),
            n_trials=ComparisonConfig.N_TRIALS,
            show_progress_bar=True
        )
        
        return study
    
    def train_final_model(self, model_name: str, best_params):
        """ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹"""
        print(f"\nä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆ {model_name} æ¨¡å‹...")
        
        # è®¾ç½®éšæœºç§å­
        set_all_seeds(ComparisonConfig.RANDOM_STATE)
        
        # åˆ›å»ºæ¨¡å‹ï¼ˆæ‰‹åŠ¨è®¾ç½®å‚æ•°ï¼‰
        if model_name == 'svm':
            model = svm.SVC(
                C=best_params['C'],
                kernel=best_params['kernel'],
                gamma=best_params['gamma'],
                random_state=ComparisonConfig.RANDOM_STATE,
                probability=True,
                class_weight='balanced'
            )
        elif model_name == 'random_forest':
            model = RandomForestClassifier(
                n_estimators=best_params['n_estimators'],
                max_depth=best_params['max_depth'],
                min_samples_split=best_params['min_samples_split'],
                criterion=best_params['criterion'],
                random_state=ComparisonConfig.RANDOM_STATE,
                class_weight='balanced'
            )
        elif model_name == 'decision_tree':
            model = DecisionTreeClassifier(
                max_depth=best_params['max_depth'],
                min_samples_split=best_params['min_samples_split'],
                criterion=best_params['criterion'],
                random_state=ComparisonConfig.RANDOM_STATE,
                class_weight='balanced'
            )
        elif model_name == 'xgboost':
            model = xgb.XGBClassifier(
                n_estimators=best_params['n_estimators'],
                max_depth=best_params['max_depth'],
                learning_rate=best_params['learning_rate'],
                random_state=ComparisonConfig.RANDOM_STATE,
                eval_metric='mlogloss'
            )
        elif model_name == 'lightgbm':
            model = lgb.LGBMClassifier(
                num_leaves=best_params['num_leaves'],
                max_depth=best_params['max_depth'],
                learning_rate=best_params['learning_rate'],
                random_state=ComparisonConfig.RANDOM_STATE,
                objective='multiclass',
                num_class=self.num_classes,
                verbose=-1
            )
        
        # åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒ
        model.fit(self.train_features, self.train_labels)
        
        return model

# ==================== ç»Ÿä¸€è®­ç»ƒå™¨ ====================
class UnifiedTrainer:
    """ç»Ÿä¸€çš„è®­ç»ƒå™¨ï¼Œæ”¯æŒä¸‰ç§æ–¹æ³•"""
    
    def __init__(self, model_type: str, train_features, train_labels, test_features, test_labels, train_df, base_processor):
        self.model_type = model_type
        self.train_features = train_features
        self.train_labels = train_labels
        self.test_features = test_features
        self.test_labels = test_labels
        self.device = ComparisonConfig.DEVICE
        self.train_df = train_df
        self.base_processor = base_processor
        self.input_size = train_features.shape[1]
        self.num_classes = len(np.unique(train_labels))
        
        print(f"åˆå§‹åŒ– {model_type} è®­ç»ƒå™¨:")
        print(f"  è®¾å¤‡: {self.device}")
        print(f"  è¾“å…¥ç»´åº¦: {self.input_size}")
        print(f"  ç±»åˆ«æ•°: {self.num_classes}")
        print(f"  äº¤å‰éªŒè¯: {ComparisonConfig.N_SPLITS}æŠ˜") 
        
    def _prepare_fold_data(self, train_idx, val_idx):
        vocab = self.base_processor.vocabulary if self.base_processor else ComparisonConfig.VOCABULARY
        proc = DataProcessor(vocabulary=vocab)
        df_tr = self.train_df.iloc[train_idx]
        df_va = self.train_df.iloc[val_idx]
        Xtr = proc.extract_features(df_tr, fit_transform=True)
        ytr = df_tr["label"].to_numpy(np.int64)
        Xva = proc.extract_features(df_va, fit_transform=False)
        yva = df_va["label"].to_numpy(np.int64)
        return Xtr, ytr, Xva, yva    

    
    def create_model(self, trial):
        """æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºæ¨¡å‹"""
        # é€šç”¨å‚æ•°
        hidden_size = trial.suggest_int('hidden_size', 64, 256)
        embedding_size = trial.suggest_int('embedding_size', 32, 128)
        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)
        
        if self.model_type == 'metric_learning':
            return MetricLearningModel(
                input_size=self.input_size,
                hidden_size=hidden_size,
                embedding_size=embedding_size,
                num_classes=self.num_classes,
                dropout_rate=dropout_rate,
                device=self.device
            )
        
        elif self.model_type == 'matching_network':
            use_fce = trial.suggest_categorical('use_fce', [True, False])
            lstm_hidden_size = trial.suggest_int('lstm_hidden_size', 16, 64) if use_fce else 32
            
            return MatchingNetwork(
                input_size=self.input_size,
                hidden_size=hidden_size,
                embedding_size=embedding_size,
                num_classes=self.num_classes,
                dropout_rate=dropout_rate,
                use_fce=use_fce,
                lstm_hidden_size=lstm_hidden_size,
                device=self.device
            )
        
        elif self.model_type == 'prototypical_network':
            return PrototypicalNetwork(
                input_size=self.input_size,
                hidden_size=hidden_size,
                embedding_size=embedding_size,
                num_classes=self.num_classes,
                dropout_rate=dropout_rate,
                device=self.device
            )

    def _load_metric_backbone_weights_(self, model, trained_models_dict):
        """
        train_metric_learning_fold/train_metric_learning è¿”å›çš„ models(dict)
        ä¸­çš„ trunk/embedder æƒé‡æ‹·å›åˆ°ä¼ å…¥çš„ model.backbone.* ä¸Šã€‚
        """
        if not hasattr(model, "backbone"):
            raise RuntimeError("Model has no backbone to load weights into.")

        if not hasattr(model.backbone, "trunk") or not hasattr(model.backbone, "embedder"):
            raise RuntimeError("Backbone missing trunk/embedder modules.")

        model.backbone.trunk.load_state_dict(trained_models_dict["trunk"].state_dict())
        model.backbone.embedder.load_state_dict(trained_models_dict["embedder"].state_dict())
        return model
    def train_few_shot_fold(self, model, trial, train_features, train_labels):
        """ä¸ºå•ä¸ªfoldè®­ç»ƒFew-Shot Learningæ¨¡å‹"""
        # è®­ç»ƒå‚æ•°
        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)
        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)
        num_episodes = trial.suggest_int('num_episodes', 50, 200)  # å‡å°‘episodes
        
        # åˆ›å»ºFew-Shotæ•°æ®é›†
        train_dataset = FewShotDataset(
            train_features, train_labels,
            n_way=ComparisonConfig.N_WAY,
            n_support=ComparisonConfig.N_SUPPORT,
            n_query=ComparisonConfig.N_QUERY,
            device=self.device
        )
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        
        # è®­ç»ƒå¾ªç¯
        model.train()
        for episode in range(num_episodes):
            # è·å–ä¸€ä¸ªepisode
            support_features, support_labels, query_features, query_labels = train_dataset[episode]
            
            # å‰å‘ä¼ æ’­
            predictions = model(support_features, support_labels, query_features, query_labels)
            
            # è®¡ç®—æŸå¤±
            loss = F.cross_entropy(predictions, query_labels)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        return model
    
    def _extract_embeddings(self, model, features, batch_size: int = 128):
        """
        ä½¿ç”¨ Few-Shot/MetricLearning æ¨¡å‹çš„ backbone æå–é€æ ·æœ¬ embeddingï¼ˆnumpyï¼‰
        - MatchingNetwork / PrototypicalNetwork: model.backbone(x) -> (trunk, emb)
        - MetricLearningModel: model.backbone.trunk(x) -> trunk; embedder(trunk) -> emb
        """
        device = self.device
        if hasattr(features, 'values'):
            features = features.values
        X = torch.from_numpy(features).float()
        if device != 'cpu':
            X = X.to(device)

        model.eval()
        embs = []
        with torch.no_grad():
            for i in range(0, len(X), batch_size):
                xb = X[i:i+batch_size]
                # å…¼å®¹ä¸‰ç§æ¨¡å‹
                if hasattr(model, "backbone") and hasattr(model.backbone, "embedder"):
                    if hasattr(model.backbone, "trunk"):
                        trunk_out = model.backbone.trunk(xb)
                        z = model.backbone.embedder(trunk_out)
                    else:
                        _, z = model.backbone(xb)  # fallback
                else:
                    _, z = model.backbone(xb)  # Matching/Proto 
                embs.append(z.detach().cpu())
        return torch.cat(embs, dim=0).numpy()

    def evaluate_few_shot_samplewise(self, model, clf_type: str = "logreg"):
        """
        åŸºäº backbone embedding çš„ sample-wise çº¿æ€§è¯„ä¼°ï¼š
        åœ¨è®­ç»ƒé›† embedding ä¸Šæ‹ŸåˆLinearSVMï¼Œåœ¨æµ‹è¯•é›† embedding ä¸Šè¯„ä¼°ã€‚
        é€‚ç”¨äº MatchingNet / ProtoNet / MetricLearningModel.
        """
        from sklearn.linear_model import LogisticRegression
        from sklearn.svm import LinearSVC
        from sklearn.preprocessing import StandardScaler
        from sklearn.pipeline import Pipeline
        from sklearn.metrics import accuracy_score, f1_score

        train_embs = self._extract_embeddings(model, self.train_features)
        test_embs  = self._extract_embeddings(model, self.test_features)

        y_train = self.train_labels if not hasattr(self.train_labels, 'values') else self.train_labels.values
        y_test  = self.test_labels  if not hasattr(self.test_labels, 'values')  else self.test_labels.values

        if clf_type == "svm":
            clf = Pipeline([
                ("scaler", StandardScaler(with_mean=True)),
                ("clf", LinearSVC(C=1.0, random_state=ComparisonConfig.RANDOM_STATE))
            ])
        else:
            clf = Pipeline([
                ("scaler", StandardScaler(with_mean=True)),
                ("clf", LogisticRegression(
                    C=1.0, max_iter=1000, random_state=ComparisonConfig.RANDOM_STATE, multi_class='auto'
                ))
            ])

        clf.fit(train_embs, y_train)
        y_pred = clf.predict(test_embs)
        acc = accuracy_score(y_test, y_pred)
        f1  = f1_score(y_test, y_pred, average='weighted')
        return acc, f1
    def evaluate_samplewise_on_fold(self, model, 
                                    fold_train_features, fold_train_labels,
                                    fold_val_features, fold_val_labels,
                                    clf_type: str = "logreg"):
        """
        â€”â€” æŠ˜å†…ï¼ˆCVï¼‰sample-wise è¯„ä¼°ï¼šlinear probe â€”â€” 
        1) ç”¨æœ¬æŠ˜è®­ç»ƒå¥½çš„ backbone æŠ½å– train/val çš„ embedding
        2) åœ¨ train-emb ä¸Šæ‹Ÿåˆ LogReg/LinearSVM
        3) åœ¨ val-emb ä¸Šåšé€æ ·æœ¬é¢„æµ‹ â†’ è¿”å› (acc, f1)
        """
        from sklearn.linear_model import LogisticRegression
        from sklearn.svm import LinearSVC
        from sklearn.preprocessing import StandardScaler
        from sklearn.pipeline import Pipeline
        from sklearn.metrics import accuracy_score, f1_score

        # to numpy
        Xtr = fold_train_features if not hasattr(fold_train_features, 'values') else fold_train_features.values
        Xva = fold_val_features   if not hasattr(fold_val_features, 'values')   else fold_val_features.values
        ytr = fold_train_labels if not hasattr(fold_train_labels, 'values') else fold_train_labels.values
        yva = fold_val_labels   if not hasattr(fold_val_labels, 'values')   else fold_val_labels.values

        # 1) æå– embeddingï¼ˆä½¿ç”¨æœ¬æŠ˜è®­ç»ƒåçš„ backboneï¼‰
        def _extract(model, X, bs=128):
            T = torch.from_numpy(X).float().to(self.device)
            embs = []
            model.eval()
            with torch.no_grad():
                for i in range(0, len(T), bs):
                    xb = T[i:i+bs]
                    # å…¼å®¹ Metric/Matching/Protoï¼šéƒ½æœ‰ SharedBackbone
                    if hasattr(model, "backbone") and hasattr(model.backbone, "trunk"):
                        trunk = model.backbone.trunk(xb)
                        z = model.backbone.embedder(trunk)
                    else:
                        _, z = model.backbone(xb)
                    embs.append(z.detach().cpu())
            return torch.cat(embs, 0).numpy()

        tr_emb = _extract(model, Xtr)
        va_emb = _extract(model, Xva)

        # 2) çº¿æ€§åˆ†ç±»å™¨
        if clf_type == "svm":
            clf = Pipeline([
                ("scaler", StandardScaler(with_mean=True)),
                ("clf", LinearSVC(C=1.0, random_state=ComparisonConfig.RANDOM_STATE))
            ])
        else:
            clf = Pipeline([
                ("scaler", StandardScaler(with_mean=True)),
                ("clf", LogisticRegression(
                    C=1.0, max_iter=1000, random_state=ComparisonConfig.RANDOM_STATE, multi_class='auto'
                ))
            ])

        # 3) æ‹Ÿåˆå¹¶åœ¨éªŒè¯é›†è¯„ä¼°ï¼ˆæ— æ³„æ¼ï¼‰
        clf.fit(tr_emb, ytr)
        y_pred = clf.predict(va_emb)
        acc = accuracy_score(yva, y_pred)
        f1  = f1_score(yva, y_pred, average='weighted')
        return acc, f1


    def _evaluate_metric_fold_direct(self, models, Xtr, ytr, Xva, yva):
        from sklearn.linear_model import LogisticRegression
        from sklearn.preprocessing import StandardScaler
        from sklearn.pipeline import Pipeline
        from sklearn.metrics import accuracy_score, f1_score
        
        # æå–è®­ç»ƒé›† embedding
        train_embs = self._extract_embeddings_from_models(models, Xtr)
        val_embs = self._extract_embeddings_from_models(models, Xva)
        
        # æ£€æŸ¥ embedding
        print(f"\n[DEBUG] Embedding æ£€æŸ¥:")
        print(f"  è®­ç»ƒé›† embedding shape: {train_embs.shape}")
        print(f"  éªŒè¯é›† embedding shape: {val_embs.shape}")
        print(f"  è®­ç»ƒé›† embedding èŒƒå›´: [{train_embs.min():.4f}, {train_embs.max():.4f}]")
        print(f"  è®­ç»ƒé›† embedding æ˜¯å¦å…¨é›¶: {np.allclose(train_embs, 0)}")
        print(f"  è®­ç»ƒé›† embedding æ˜¯å¦æœ‰ NaN: {np.isnan(train_embs).any()}")
        print(f"  è®­ç»ƒé›† embedding æ˜¯å¦æœ‰ Inf: {np.isinf(train_embs).any()}")
        
        # è½¬æ¢æ ‡ç­¾
        ytr_np = ytr if not hasattr(ytr, 'values') else ytr.values
        yva_np = yva if not hasattr(yva, 'values') else yva.values
        
        print(f"  è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(ytr_np)}")
        print(f"  éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(yva_np)}")
        
        # è®­ç»ƒçº¿æ€§åˆ†ç±»å™¨
        clf = Pipeline([
            ("scaler", StandardScaler(with_mean=True)),
            ("clf", LogisticRegression(
                C=1.0, max_iter=1000, 
                random_state=ComparisonConfig.RANDOM_STATE, 
                multi_class='auto'
            ))
        ])
        
        try:
            clf.fit(train_embs, ytr_np)
            y_pred = clf.predict(val_embs)
            
            acc = accuracy_score(yva_np, y_pred)
            f1 = f1_score(yva_np, y_pred, average='weighted')
            
            print(f"  éªŒè¯é›†é¢„æµ‹åˆ†å¸ƒ: {np.bincount(y_pred)}")
            print(f"  å‡†ç¡®ç‡: {acc:.4f}, F1: {f1:.4f}")
            
            return acc, f1
        except Exception as e:
            print(f"[ERROR] çº¿æ€§åˆ†ç±»å™¨è®­ç»ƒå¤±è´¥: {e}")
            return 0.0, 0.0

    def _extract_embeddings_from_models(self, models, X, batch_size=128):
        """
        ä»åº¦é‡å­¦ä¹ çš„ models dict ä¸­æå– embedding
        """
        device = self.device
        if hasattr(X, 'values'):
            X = X.values
        
        X_tensor = torch.from_numpy(X).float().to(device)
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        models['trunk'].eval()
        models['embedder'].eval()
        
        embeddings = []
        with torch.no_grad():
            for i in range(0, len(X_tensor), batch_size):
                xb = X_tensor[i:i+batch_size]
                trunk_out = models['trunk'](xb)
                emb = models['embedder'](trunk_out)
                embeddings.append(emb.cpu())
        result = torch.cat(embeddings, dim=0).numpy()
        
        # æ£€æŸ¥
        if np.isnan(result).any() or np.isinf(result).any():
            print(f"[ERROR] Embedding åŒ…å« NaN æˆ– Inf!")
            return np.zeros_like(result)
        
        if np.allclose(result, 0):
            print(f"[WARNING] Embedding å…¨ä¸ºé›¶!")
        
        return torch.cat(embeddings, dim=0).numpy()
    def objective(self, trial):
        """ä¼˜åŒ–ç›®æ ‡å‡½æ•° - ä½¿ç”¨æ­£ç¡®çš„äº”æŠ˜äº¤å‰éªŒè¯"""
        # ä½¿ç”¨å›ºå®šéšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
        set_all_seeds(ComparisonConfig.RANDOM_STATE)
        
        try:
            # ğŸ”¥ æ­£ç¡®ä½¿ç”¨äº”æŠ˜äº¤å‰éªŒè¯
            cv_scores = []
            cv_f1_scores = []
            skf = ComparisonConfig.get_cv_splitter()
            
            print(f"\nå¼€å§‹ {ComparisonConfig.N_SPLITS} æŠ˜äº¤å‰éªŒè¯...")
            
            # å¯¹æ¯ä¸€æŠ˜è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°
            for fold, (train_idx, val_idx) in enumerate(skf.split(self.train_features, self.train_labels)):
                print(f"  æ­£åœ¨å¤„ç†ç¬¬ {fold + 1}/{ComparisonConfig.N_SPLITS} æŠ˜...")
                # Xtr, ytr, Xva, yva = self._prepare_fold_data(train_idx, val_idx)
                # åˆ›å»ºæ¨¡å‹
                model = self.create_model(trial)
                fold_processor = DataProcessor(self.base_processor.vocabulary)
                df_train = self.train_df.iloc[train_idx]
                df_val = self.train_df.iloc[val_idx]
                X_train = fold_processor.extract_features(df_train, fit_transform=True)
                X_val = fold_processor.extract_features(df_val, fit_transform=False)
                y_train = df_train["label"].to_numpy(np.int64)
                y_val = df_val["label"].to_numpy(np.int64)
                # æ ¹æ®æ¨¡å‹ç±»å‹è®­ç»ƒ
                if self.model_type == "metric_learning":
                    # è®­ç»ƒåº¦é‡å­¦ä¹ ï¼ˆå¾—åˆ° dict: {'trunk', 'embedder', ...}ï¼‰
                    trained_models = self.train_metric_learning_fold(model, trial, X_train, y_train)


                    # ç”¨â€œå·²æ›´æ–°æƒé‡â€çš„ model åš linear-probe çš„æŠ˜å†…è¯„ä¼°
                    fold_score, fold_f1 = self._evaluate_metric_fold_direct(
                        trained_models, X_train, y_train, X_val, y_val
                    )

                else:
                    # Few-shot (Matching/Proto): è®­ç»ƒè¿”å›çš„å°±æ˜¯â€œå·²æ›´æ–°â€çš„æ¨¡å‹
                    trained_model = self.train_few_shot_fold(model, trial, X_train, y_train)

                    # ç›´æ¥ç”¨è®­ç»ƒåçš„æ¨¡å‹è¯„ä¼°ï¼ˆsample-wise linear-probeï¼‰
                    fold_score, fold_f1 = self.evaluate_samplewise_on_fold(
                        trained_model, X_train, y_train, X_val, y_val, clf_type="logreg"
                    )
                cv_scores.append(fold_score)
                cv_f1_scores.append(fold_f1)
                print(f"    ç¬¬ {fold + 1} æŠ˜å‡†ç¡®ç‡: {fold_score:.4f}")
                print(f"    ç¬¬ {fold + 1} æŠ˜F1: {fold_f1:.4f}")
                
                # æ¯æŠ˜åè¿›è¡Œå‰ªæåˆ¤æ–­
                trial.report(fold_score, fold)
                if trial.should_prune():
                    print(f"    è¯•éªŒåœ¨ç¬¬ {fold + 1} æŠ˜è¢«å‰ªæ")
                    raise optuna.TrialPruned()
            
            # è®¡ç®—å¹³å‡äº¤å‰éªŒè¯åˆ†æ•°
            avg_score = np.mean(cv_scores)
            std_score = np.std(cv_scores)
            
            trial.set_user_attr('fold_scores', cv_scores)
            trial.set_user_attr('fold_f1_scores', cv_f1_scores)
            print(f"  äº¤å‰éªŒè¯ç»“æœ: {avg_score:.4f} Â± {std_score:.4f}")
            print(f"  å„æŠ˜åˆ†æ•°: {[f'{score:.4f}' for score in cv_scores]}")
            print(f"  å„æŠ˜F1: {[f'{score:.4f}' for score in cv_f1_scores]}")
            
            return avg_score
            
        except optuna.TrialPruned:
            raise
        except Exception as e:
            print(f"è¯•éªŒå¤±è´¥: {e}")
            return 0.0

    def train_metric_learning_fold(self, model, trial, train_features, train_labels):
        """ä¸ºå•ä¸ªfoldè®­ç»ƒåº¦é‡å­¦ä¹ æ¨¡å‹"""
        # è®­ç»ƒå‚æ•°
        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)
        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)
        batch_size = trial.suggest_int('batch_size', 4, 16)
        num_epochs = trial.suggest_int('num_epochs', 20, 60)  # å‡å°‘epochä»¥ä¾¿å¿«é€Ÿäº¤å‰éªŒè¯
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = MedicalDataset(train_features, train_labels, device=self.device)
        
        # åˆ†ç¦»æ¨¡å‹ç»„ä»¶
        models = {
            "trunk": model.backbone.trunk,
            "embedder": model.backbone.embedder,
            "classifier": model.classifier
        }
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizers = {
            "trunk_optimizer": torch.optim.Adam(
                model.backbone.trunk.parameters(), lr=learning_rate, weight_decay=weight_decay
            ),
            "embedder_optimizer": torch.optim.Adam(
                model.backbone.embedder.parameters(), lr=learning_rate, weight_decay=weight_decay
            ),
            "classifier_optimizer": torch.optim.Adam(
                model.classifier.parameters(), lr=learning_rate, weight_decay=weight_decay
            )
        }
        
        # ç®€åŒ–çš„æŸå¤±å‡½æ•°å’ŒæŒ–æ˜å™¨ï¼ˆä¸ºäº†å¿«é€Ÿäº¤å‰éªŒè¯ï¼‰
        loss_funcs = {
            "metric_loss": losses.TripletMarginLoss(margin=0.2),
            "classifier_loss": nn.CrossEntropyLoss()
        }
        
        mining_funcs = {"tuple_miner": miners.TripletMarginMiner()}
        loss_weights = {"metric_loss": 1.0, "classifier_loss": 0.5}
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = trainers.TrainWithClassifier(
            models=models,
            optimizers=optimizers,
            batch_size=batch_size,
            loss_funcs=loss_funcs,
            dataset=train_dataset,
            mining_funcs=mining_funcs,
            dataloader_num_workers=0,
            loss_weights=loss_weights
        )
        
        # è®­ç»ƒ
        trainer.train(num_epochs=num_epochs)
        
        return models
    
    def evaluate_metric_learning_fold(self, models, val_features, val_labels):
        """è¯„ä¼°å•ä¸ªfoldçš„åº¦é‡å­¦ä¹ æ¨¡å‹ï¼Œè¿”å› (accuracy, f1)"""
        val_dataset = MedicalDataset(val_features, val_labels, device=self.device)
        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)

        for m in models.values():
            m.eval()

        all_preds, all_true = [], []
        with torch.no_grad():
            for x, y in val_loader:
                x = x.to(self.device)
                y = y.to(self.device)
                z = models['embedder'](models['trunk'](x))
                logits = models['classifier'](z)
                pred = logits.argmax(dim=1)
                all_preds.extend(pred.cpu().numpy())
                all_true.extend(y.cpu().numpy())

        acc = float((np.array(all_preds) == np.array(all_true)).mean()) if all_true else 0.0
        f1 = f1_score(all_true, all_preds, average='weighted') if all_true else 0.0
        return acc, f1
    
    def evaluate_few_shot_fold(self, model, val_features, val_labels):
        """è¯„ä¼°å•ä¸ªfoldçš„Few-Shot Learningæ¨¡å‹"""
        # åˆ›å»ºéªŒè¯æ•°æ®é›†
        val_dataset = FewShotDataset(
            val_features, val_labels,
            n_way=ComparisonConfig.N_WAY,
            n_support=ComparisonConfig.N_SUPPORT,
            n_query=ComparisonConfig.N_QUERY,
            device=self.device
        )
        
        model.eval()
        correct = 0
        total = 0
        all_predictions = []
        all_true_labels = []
        
        with torch.no_grad():
            # æµ‹è¯•å¤šä¸ªepisode
            for episode in range(200):  # éªŒè¯æ—¶ä½¿ç”¨è¾ƒå°‘episode
                support_features, support_labels, query_features, query_labels = val_dataset[episode]
                
                predictions = model(support_features, support_labels, query_features, query_labels)
                _, predicted = torch.max(predictions.data, 1)
                
                total += query_labels.size(0)
                correct += (predicted == query_labels).sum().item()
                
                # æ”¶é›†é¢„æµ‹ç»“æœç”¨äºF1è®¡ç®—
                all_predictions.extend(predicted.cpu().numpy())
                all_true_labels.extend(query_labels.cpu().numpy())
        
        accuracy = correct / total
        f1 = f1_score(all_true_labels, all_predictions, average='weighted')
        return accuracy, f1

    def train_final_model(self, best_params):
        """ä½¿ç”¨æœ€ä½³å‚æ•°åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹"""
        print(f"\nä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆ {self.model_type} æ¨¡å‹...")
        
        # è®¾ç½®éšæœºç§å­
        set_all_seeds(ComparisonConfig.RANDOM_STATE)
        
        if self.model_type == 'metric_learning':
            return self._train_final_metric_learning(best_params)
        else:
            return self._train_final_few_shot(best_params)
    
    def _train_final_metric_learning(self, best_params):
        """è®­ç»ƒæœ€ç»ˆçš„åº¦é‡å­¦ä¹ æ¨¡å‹ - æ‰‹åŠ¨è®­ç»ƒå¾ªç¯"""
        print(f"\nä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆåº¦é‡å­¦ä¹ æ¨¡å‹...")
        # åˆ›å»ºæ¨¡å‹
        model = MetricLearningModel(
            input_size=self.input_size,
            hidden_size=best_params['hidden_size'],
            embedding_size=best_params['embedding_size'],
            num_classes=self.num_classes,
            dropout_rate=best_params['dropout_rate'],
            device=self.device
        )
        
        # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
        train_dataset = MedicalDataset(self.train_features, self.train_labels, device=self.device)
        train_loader = torch.utils.data.DataLoader(
            train_dataset, 
            batch_size=best_params.get('batch_size'), 
            shuffle=True,
            drop_last=len(train_dataset) > best_params['batch_size']
        )
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=best_params['learning_rate'], 
            weight_decay=best_params['weight_decay']
        )
        
        # æŸå¤±å‡½æ•°
        metric_loss_fn = losses.TripletMarginLoss(margin=0.2)
        class_loss_fn = nn.CrossEntropyLoss()
        miner = miners.TripletMarginMiner()
        
        # æ‰‹åŠ¨è®­ç»ƒå¾ªç¯
        model.train()
        num_epochs = best_params['num_epochs']
        
        print(f"è®­ç»ƒ {num_epochs} ä¸ªepochs...")
        for epoch in range(num_epochs):
            epoch_losses = []
            
            for batch_features, batch_labels in train_loader:
                batch_features = batch_features.to(self.device)
                batch_labels = batch_labels.to(self.device)
                
                # å‰å‘ä¼ æ’­
                trunk_output, embeddings, logits = model(batch_features)
                
                # è®¡ç®—æŸå¤±
                hard_pairs = miner(embeddings, batch_labels)
                metric_loss = metric_loss_fn(embeddings, batch_labels, hard_pairs)
                class_loss = class_loss_fn(logits, batch_labels)
                total_loss = metric_loss + 0.5 * class_loss
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                total_loss.backward()
                optimizer.step()
                
                epoch_losses.append(total_loss.item())
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0 or epoch == 0:
                avg_loss = np.mean(epoch_losses)
                print(f"  Epoch {epoch+1}/{num_epochs}: Loss={avg_loss:.4f}")
        
        print("è®­ç»ƒå®Œæˆï¼")
        
        # è®¾ç½®ä¸ºevalæ¨¡å¼å¹¶è¿”å›åŸå§‹nn.Module
        model.eval()
        
        models = {
            "trunk": model.backbone.trunk,
            "embedder": model.backbone.embedder,
            "classifier": model.classifier
        }
        
        # ç¡®ä¿æ‰€æœ‰ç»„ä»¶éƒ½æ˜¯evalæ¨¡å¼
        for m in models.values():
            m.eval()
        
        return models
    
    def _train_final_few_shot(self, best_params):
        """è®­ç»ƒæœ€ç»ˆçš„Few-Shot Learningæ¨¡å‹"""
        # åˆ›å»ºæ¨¡å‹
        if self.model_type == 'matching_network':
            model = MatchingNetwork(
                input_size=self.input_size,
                hidden_size=best_params['hidden_size'],
                embedding_size=best_params['embedding_size'],
                num_classes=self.num_classes,
                dropout_rate=best_params['dropout_rate'],
                use_fce=best_params['use_fce'],
                lstm_hidden_size=best_params.get('lstm_hidden_size', 32),
                device=self.device
            )
        else:  # prototypical_network
            model = PrototypicalNetwork(
                input_size=self.input_size,
                hidden_size=best_params['hidden_size'],
                embedding_size=best_params['embedding_size'],
                num_classes=self.num_classes,
                dropout_rate=best_params['dropout_rate'],
                device=self.device
            )
        
        # åˆ›å»ºFew-Shotæ•°æ®é›†
        train_dataset = FewShotDataset(
            self.train_features, self.train_labels,
            n_way=ComparisonConfig.N_WAY,
            n_support=ComparisonConfig.N_SUPPORT,
            n_query=ComparisonConfig.N_QUERY,
            device=self.device
        )
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=best_params['learning_rate'], 
            weight_decay=best_params['weight_decay']
        )
        
        # è®­ç»ƒå¾ªç¯
        model.train()
        num_episodes = best_params.get('num_episodes', 200)
        
        print(f"è®­ç»ƒ {num_episodes} ä¸ªepisodes...")
        for episode in range(num_episodes):
            support_features, support_labels, query_features, query_labels = train_dataset[episode]
            
            predictions = model(support_features, support_labels, query_features, query_labels)
            loss = F.cross_entropy(predictions, query_labels)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            if (episode + 1) % 50 == 0:
                print(f"  Episode {episode + 1}/{num_episodes}, Loss: {loss.item():.4f}")
        
        return model
    
    def train_few_shot(self, best_params):
        """ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒFew-Shot Learningæ¨¡å‹"""
        # åˆ›å»ºæ¨¡å‹
        model = self.create_model_with_params(best_params)
        
        # è®­ç»ƒå‚æ•°
        learning_rate = best_params['learning_rate']
        weight_decay = best_params['weight_decay']
        num_episodes = best_params['num_episodes']
        
        # åˆ›å»ºFew-Shotæ•°æ®é›†
        train_dataset = FewShotDataset(
            self.train_features, self.train_labels,
            n_way=ComparisonConfig.N_WAY,
            n_support=ComparisonConfig.N_SUPPORT,
            n_query=ComparisonConfig.N_QUERY,
            device=self.device
        )
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
        
        # è®­ç»ƒå¾ªç¯
        model.train()
        for episode in range(num_episodes):
            # è·å–ä¸€ä¸ªepisode
            support_features, support_labels, query_features, query_labels = train_dataset[episode]
            
            # å‰å‘ä¼ æ’­
            predictions = model(support_features, support_labels, query_features, query_labels)
            
            # è®¡ç®—æŸå¤±
            loss = F.cross_entropy(predictions, query_labels)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        return model
    
    def train_metric_learning(self, best_params):
        """ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒåº¦é‡å­¦ä¹ æ¨¡å‹"""
        # åˆ›å»ºæ¨¡å‹
        model = self.create_model_with_params(best_params)
        
        # è®­ç»ƒå‚æ•°
        learning_rate = best_params['learning_rate']
        weight_decay = best_params['weight_decay']
        batch_size = best_params['batch_size']
        num_epochs = best_params['num_epochs']
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        train_dataset = MedicalDataset(self.train_features, self.train_labels, device=self.device)
        train_loader = torch.utils.data.DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            drop_last=len(train_dataset) > batch_size
        )
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(
            model.parameters(), 
            lr=learning_rate, 
            weight_decay=weight_decay
        )
        
        # æŸå¤±å‡½æ•°
        metric_loss_fn = losses.TripletMarginLoss(margin=0.2)
        class_loss_fn = nn.CrossEntropyLoss()
        miner = miners.TripletMarginMiner()
        
        # è®­ç»ƒå¾ªç¯
        model.train()
        for epoch in range(num_epochs):
            for batch_features, batch_labels in train_loader:
                batch_features = batch_features.to(self.device)
                batch_labels = batch_labels.to(self.device)
                
                # å‰å‘ä¼ æ’­
                trunk_output, embeddings, logits = model(batch_features)
                
                # è®¡ç®—æŸå¤±
                hard_pairs = miner(embeddings, batch_labels)
                metric_loss = metric_loss_fn(embeddings, batch_labels, hard_pairs)
                class_loss = class_loss_fn(logits, batch_labels)
                total_loss = metric_loss + 0.5 * class_loss
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                total_loss.backward()
                optimizer.step()
        
        # è®¾ç½®ä¸ºevalæ¨¡å¼
        model.eval()
        
        # è¿”å›åŸå§‹çš„nn.Moduleç»„ä»¶ï¼Œè€Œä¸æ˜¯å­—å…¸
        models = {
            "trunk": model.backbone.trunk,
            "embedder": model.backbone.embedder,
            "classifier": model.classifier
        }
        
        return models

    def evaluate_metric_learning(self, models):
        """è¯„ä¼°åº¦é‡å­¦ä¹ æ¨¡å‹"""
        test_dataset = MedicalDataset(self.test_features, self.test_labels, device=self.device)
        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)
        
        for model in models.values():
            model.eval()
        
        correct = 0
        total = 0
        
        all_preds, all_true = [], []
        with torch.no_grad():
            for batch_features, batch_labels in test_loader:
                batch_features = batch_features.to(self.device)
                batch_labels = batch_labels.to(self.device)
                
                trunk_output = models['trunk'](batch_features)
                embeddings = models['embedder'](trunk_output)
                logits = models['classifier'](embeddings)
                
                _, predicted = torch.max(logits.data, 1)
                all_preds.extend(predicted.cpu().numpy())
                all_true.extend(batch_labels.cpu().numpy())
                total += batch_labels.size(0)
                correct += (predicted == batch_labels).sum().item()
        f1 = f1_score(all_true, all_preds, average='weighted') if len(all_true) > 0 else 0.0
        return correct / total, f1
    
    def evaluate_few_shot(self, model):
        """è¯„ä¼°Few-Shot Learningæ¨¡å‹"""
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        test_dataset = FewShotDataset(
            self.test_features, self.test_labels,
            n_way=ComparisonConfig.N_WAY,
            n_support=ComparisonConfig.N_SUPPORT,
            n_query=ComparisonConfig.N_QUERY,
            device=self.device
        )
        
        model.eval()
        correct = 0
        total = 0
        all_predictions = []
        all_true_labels = []
        
        with torch.no_grad():
            for episode in range(min(100, len(test_dataset))):  # é™åˆ¶æµ‹è¯•episodesæ•°é‡
                support_features, support_labels, query_features, query_labels = test_dataset[episode]
                
                predictions = model(support_features, support_labels, query_features, query_labels)
                _, predicted = torch.max(predictions.data, 1)
                
                total += query_labels.size(0)
                correct += (predicted == query_labels).sum().item()
                
                # æ”¶é›†é¢„æµ‹ç»“æœç”¨äºF1è®¡ç®—
                all_predictions.extend(predicted.cpu().numpy())
                all_true_labels.extend(query_labels.cpu().numpy())
        
        accuracy = correct / total if total > 0 else 0.0
        f1 = f1_score(all_true_labels, all_predictions, average='weighted') if len(all_true_labels) > 0 else 0.0
        return accuracy, f1
    
    def create_model_with_params(self, params):
        """ä½¿ç”¨ç»™å®šå‚æ•°åˆ›å»ºæ¨¡å‹"""
        if self.model_type == 'metric_learning':
            return MetricLearningModel(
                input_size=self.input_size,
                hidden_size=params['hidden_size'],
                embedding_size=params['embedding_size'],
                num_classes=self.num_classes,
                dropout_rate=params['dropout_rate'],
                device=self.device
            )
        
        elif self.model_type == 'matching_network':
            return MatchingNetwork(
                input_size=self.input_size,
                hidden_size=params['hidden_size'],
                embedding_size=params['embedding_size'],
                num_classes=self.num_classes,
                dropout_rate=params['dropout_rate'],
                use_fce=params['use_fce'],
                lstm_hidden_size=params.get('lstm_hidden_size', 32),
                device=self.device
            )
        
        elif self.model_type == 'prototypical_network':
            return PrototypicalNetwork(
                input_size=self.input_size,
                hidden_size=params['hidden_size'],
                embedding_size=params['embedding_size'],
                num_classes=self.num_classes,
                dropout_rate=params['dropout_rate'],
                device=self.device
            )
    
    def optimize(self):
        """æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–"""
        print(f"å¼€å§‹ä¼˜åŒ– {self.model_type}...")
        
        # åˆ›å»ºç ”ç©¶
        study = optuna.create_study(
            direction='maximize',
            pruner=MedianPruner(n_startup_trials=ComparisonConfig.N_STARTUP_TRIALS),
            sampler=TPESampler(seed=ComparisonConfig.RANDOM_STATE),
            study_name=f'{self.model_type}_optimization'
        )
        
        # æ‰§è¡Œä¼˜åŒ–
        study.optimize(
            self.objective,
            n_trials=ComparisonConfig.N_TRIALS,
            show_progress_bar=True
        )
        
        return study

def save_comparison_results(results, studies):
    """ä¿å­˜å¯¹æ¯”ç»“æœ - å‚æ•°ä¿å­˜ä¸ºJSONï¼Œå…¶ä»–ç»“æœä¿å­˜ä¸ºCSV"""
    print("\nğŸ’¾ ä¿å­˜å¯¹æ¯”ç»“æœ...")
    
    import json
    import pandas as pd
    
    method_names = {
        # Few-Shot Learningæ–¹æ³•
        'metric_learning': 'åº¦é‡å­¦ä¹ ',
        'matching_network': 'åŒ¹é…ç½‘ç»œ', 
        'prototypical_network': 'åŸå‹ç½‘ç»œ',
        # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
        'svm': 'æ”¯æŒå‘é‡æœº',
        'random_forest': 'éšæœºæ£®æ—',
        'decision_tree': 'å†³ç­–æ ‘',
        'xgboost': 'XGBoost',
        'lightgbm': 'LightGBM'
    }
    
    # 1. ä¿å­˜æœ€ä½³å‚æ•°ä¸ºJSONæ–‡ä»¶ï¼ˆæ¯ä¸ªæ¨¡å‹å•ç‹¬ä¿å­˜ï¼‰
    print("\nä¿å­˜å„æ¨¡å‹æœ€ä½³å‚æ•°(JSONæ ¼å¼)...")
    for method, result in results.items():
        if 'error' not in result and result['best_params']:
            filename = f'best_params_{method}.json'
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(result['best_params'], f, indent=2, ensure_ascii=False)
            print(f"  {method_names.get(method, method)} å‚æ•°å·²ä¿å­˜åˆ°: {filename}")
    
    # 2. åˆ›å»ºç»“æœæ±‡æ€»CSVè¡¨
    print("\nä¿å­˜ç»“æœæ±‡æ€»(CSVæ ¼å¼)...")
    summary_data = []
    for method, result in results.items():
        if 'error' not in result:
            summary_data.append({
                'æ–¹æ³•': method_names.get(method, method),
                'æ–¹æ³•ç±»å‹': result.get('method_type', 'unknown'),
                'äº¤å‰éªŒè¯æœ€ä½³å‡†ç¡®ç‡': result['best_accuracy'],
                'äº¤å‰éªŒè¯å‡†ç¡®ç‡': result.get('cv_fold_accuracies', []),
                "äº¤å‰éªŒè¯å‡†ç¡®ç‡å‡å€¼": result.get('cv_mean', 0.0),
                'äº¤å‰éªŒè¯F1': result.get('cv_fold_f1_scores', []),
                'äº¤å‰éªŒè¯F1å‡å€¼': result.get('cv_f1_mean', 0.0),
                'å‡†ç¡®ç‡æ ‡å‡†å·®': result.get('cv_std', 0.0),
                'F1æ ‡å‡†å·®': result.get('cv_f1_std', 0.0),
                'æµ‹è¯•é›†å‡†ç¡®ç‡': result.get('test_accuracy', None),
                'è¿‡æ‹Ÿåˆå·®è·': result.get('overfitting_gap', None),
                'è¯•éªŒæ¬¡æ•°': result['n_trials']
            })
    
    summary_df = pd.DataFrame(summary_data)
    summary_df = summary_df.sort_values('äº¤å‰éªŒè¯æœ€ä½³å‡†ç¡®ç‡', ascending=False)
    summary_df.to_csv('model_performance_summary.csv', index=False, encoding='utf-8-sig')
    print("ç»“æœæ±‡æ€»å·²ä¿å­˜åˆ°: model_performance_summary.csv")
    
    # 3. åˆ›å»ºäº¤å‰éªŒè¯è¯¦ç»†åˆ†æ•°CSVè¡¨
    print("\nä¿å­˜äº¤å‰éªŒè¯è¯¦ç»†åˆ†æ•°(CSVæ ¼å¼)...")
    cv_detailed_data = []
    for method, result in results.items():
        if 'error' not in result and result.get('cv_fold_accuracies'):
            fold_accuracies = result['cv_fold_accuracies']
            fold_f1_scores = result.get('cv_fold_f1_scores', [0] * len(fold_accuracies))
            
            for fold_idx, (acc, f1) in enumerate(zip(fold_accuracies, fold_f1_scores)):
                cv_detailed_data.append({
                    'æ–¹æ³•': method_names.get(method, method),
                    'æ–¹æ³•ç±»å‹': result.get('method_type', 'unknown'),
                    'æŠ˜æ•°': fold_idx + 1,
                    'å‡†ç¡®ç‡': acc,
                    'F1åˆ†æ•°': f1
                })
    
    cv_detailed_df = pd.DataFrame(cv_detailed_data)
    cv_detailed_df.to_csv('cv_fold_scores.csv', index=False, encoding='utf-8-sig')
    print("äº¤å‰éªŒè¯è¯¦ç»†åˆ†æ•°å·²ä¿å­˜åˆ°: cv_fold_scores.csv")
    
    # 4. åˆ›å»ºå®éªŒé…ç½®CSV
    print("\nä¿å­˜å®éªŒé…ç½®(CSVæ ¼å¼)...")
    config_data = [{
        'é…ç½®é¡¹': 'è¯•éªŒæ¬¡æ•°',
        'å€¼': ComparisonConfig.N_TRIALS
    }, {
        'é…ç½®é¡¹': 'äº¤å‰éªŒè¯æŠ˜æ•°',
        'å€¼': ComparisonConfig.N_SPLITS
    }, {
        'é…ç½®é¡¹': 'Few-Shot N-Way',
        'å€¼': ComparisonConfig.N_WAY
    }, {
        'é…ç½®é¡¹': 'Few-Shot N-Support',
        'å€¼': ComparisonConfig.N_SUPPORT
    }, {
        'é…ç½®é¡¹': 'Few-Shot N-Query',
        'å€¼': ComparisonConfig.N_QUERY
    }, {
        'é…ç½®é¡¹': 'è®¾å¤‡',
        'å€¼': ComparisonConfig.DEVICE
    }, {
        'é…ç½®é¡¹': 'éšæœºç§å­',
        'å€¼': ComparisonConfig.RANDOM_STATE
    }]
    
    config_df = pd.DataFrame(config_data)
    config_df.to_csv('experiment_config.csv', index=False, encoding='utf-8-sig')
    print("å®éªŒé…ç½®å·²ä¿å­˜åˆ°: experiment_config.csv")
    
    # 5. ä¿å­˜æœ€ä½³æ–¹æ³•ä¿¡æ¯ä¸ºCSV
    print("\n   ä¿å­˜æœ€ä½³æ–¹æ³•ä¿¡æ¯(CSVæ ¼å¼)...")
    best_method = max(results.keys(), key=lambda x: results[x]['best_accuracy'])
    best_info_data = [{
        'æœ€ä½³æ–¹æ³•': method_names.get(best_method, best_method),
        'æœ€ä½³å‡†ç¡®ç‡': results[best_method]['best_accuracy'],
        'æœ€ä½³F1': results[best_method].get('cv_f1_mean', 0.0),
        'æ–¹æ³•ç±»å‹': results[best_method].get('method_type', 'unknown')
    }]
    
    best_info_df = pd.DataFrame(best_info_data)
    best_info_df.to_csv('best_method_info.csv', index=False, encoding='utf-8-sig')
    print("æœ€ä½³æ–¹æ³•ä¿¡æ¯å·²ä¿å­˜åˆ°: best_method_info.csv")
    
    # 6. ä¿å­˜æ–¹æ³•æ’åä¸ºCSV
    print("\n  ä¿å­˜æ–¹æ³•æ’å(CSVæ ¼å¼)...")
    ranking_data = []
    sorted_methods = sorted(results.keys(), key=lambda x: results[x]['best_accuracy'], reverse=True)
    for rank, method in enumerate(sorted_methods, 1):
        if 'error' not in results[method]:
            ranking_data.append({
                'æ’å': rank,
                'æ–¹æ³•': method_names.get(method, method),
                'å‡†ç¡®ç‡': results[method]['best_accuracy'],
                'F1åˆ†æ•°': results[method].get('cv_f1_mean', 0.0),
                'æ–¹æ³•ç±»å‹': results[method].get('method_type', 'unknown')
            })
    
    ranking_df = pd.DataFrame(ranking_data)
    ranking_df.to_csv('method_ranking.csv', index=False, encoding='utf-8-sig')
    print("æ–¹æ³•æ’åå·²ä¿å­˜åˆ°: method_ranking.csv")
    
    print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {method_names.get(best_method, best_method)}")
    print("="*60)
    print("ä¿å­˜çš„æ–‡ä»¶åˆ—è¡¨:")
    print("JSONæ–‡ä»¶ (æœ€ä½³å‚æ•°):")
    for method in results.keys():
        if 'error' not in results[method]:
            print(f"  - best_params_{method}.json")
    print("\nCSVæ–‡ä»¶ (å…¶ä»–ç»“æœ):")
    print("  - model_performance_summary.csv (æ€§èƒ½æ±‡æ€»)")
    print("  - cv_fold_scores.csv (äº¤å‰éªŒè¯è¯¦ç»†åˆ†æ•°)")
    print("  - experiment_config.csv (å®éªŒé…ç½®)")
    print("  - best_method_info.csv (æœ€ä½³æ–¹æ³•ä¿¡æ¯)")
    print("  - method_ranking.csv (æ–¹æ³•æ’å)")

"""è¿è¡Œæ‰€æœ‰æ–¹æ³•çš„å¯¹æ¯”å®éªŒ"""
print("="*80)

# è®¾ç½®å…¨å±€éšæœºç§å­
set_all_seeds(ComparisonConfig.RANDOM_STATE)

# åŠ è½½æ•°æ®
print("\nåŠ è½½æ•°æ®...")
train_data = pd.read_csv(ComparisonConfig.TRAIN_DATA_PATH)
test_data = pd.read_csv(ComparisonConfig.TEST_DATA_PATH)

print(f"è®­ç»ƒæ•°æ®: {len(train_data)} æ ·æœ¬")
print(f"æµ‹è¯•æ•°æ®: {len(test_data)} æ ·æœ¬")

# æ•°æ®é¢„å¤„ç†
print("\næ•°æ®é¢„å¤„ç†...")
processor = DataProcessor(ComparisonConfig.VOCABULARY)

# æå–ç‰¹å¾
train_features = processor.extract_features(train_data, fit_transform=True)
test_features = processor.extract_features(test_data, fit_transform=False)

# æå–æ ‡ç­¾
train_labels = train_data["label"].to_numpy().astype(np.int64)
test_labels = test_data["label"].to_numpy().astype(np.int64)

print(f"ç‰¹å¾ç»´åº¦: {train_features.shape[1]}")
print(f"ç±»åˆ«æ•°é‡: {len(np.unique(train_labels))}")

# å­˜å‚¨ç»“æœ
results = {}
studies = {}

few_shot_methods = ['metric_learning', 'matching_network', 'prototypical_network']
traditional_ml_methods = ['svm', 'random_forest', 'decision_tree', 'xgboost', 'lightgbm']

method_names = {
    # Few-Shot Learningæ–¹æ³•
    'metric_learning': 'åº¦é‡å­¦ä¹ ',
    'matching_network': 'åŒ¹é…ç½‘ç»œ', 
    'prototypical_network': 'åŸå‹ç½‘ç»œ',
    # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
    'svm': 'æ”¯æŒå‘é‡æœº',
    'random_forest': 'éšæœºæ£®æ—',
    'decision_tree': 'å†³ç­–æ ‘',
    'xgboost': 'XGBoost',
    'lightgbm': 'LightGBM'
}

# å…ˆä¼˜åŒ–Few-Shot Learningæ–¹æ³•
print("\n" + "="*80)
print("ç¬¬ä¸€é˜¶æ®µï¼šFew-Shot Learningæ–¹æ³•ä¼˜åŒ–")
print("="*80)

for method in few_shot_methods:
    print(f"\n{'='*60}")
    print(f"å¼€å§‹ä¼˜åŒ–: {method_names[method]}")
    print(f"{'='*60}")
    
    try:
        # åˆ›å»ºFew-Shot Learningè®­ç»ƒå™¨
        trainer = UnifiedTrainer(method, train_features, train_labels, test_features, test_labels, train_data, processor)
        
        # æ‰§è¡Œä¼˜åŒ–
        study = trainer.optimize()
        best_params = study.best_params
        final_model = trainer.train_final_model(best_params)
        fold_scores = study.best_trial.user_attrs.get('fold_scores', None)
        fold_f1_scores = study.best_trial.user_attrs.get('fold_f1_scores', None)
        class_names = ['L', 'M', 'S']
        inspector = PerClassInspector(
                    trainer=trainer,
                    trained=final_model,
                    class_names=class_names,
                    model_name=trainer.model_type,
                    random_state=0,
                    )
        inspector.run_all(
                    save_dir=f'figs/inspect/{trainer.model_type}',
                    distance_metric='cosine', # æˆ– 'euclidean'
                    embed_vis='umap', # æ—  umap æ—¶è‡ªåŠ¨å›é€€ t-SNE
                    do_matching_heatmap=True, # ä»… matching_network æ—¶æœ‰æ•ˆ
                    episode_way=3, episode_support=2, episode_query=2,
                    topk_list=[1, 3, 5],
                    )
        results[method] = {
            'best_accuracy': study.best_value,
            'best_params': study.best_params,
            'n_trials': len(study.trials),
            'cv_fold_accuracies': fold_scores,
            'cv_mean': float(np.mean(fold_scores)) if fold_scores else None,
            'cv_std': float(np.std(fold_scores)) if fold_scores else None,
            'cv_fold_f1_scores': fold_f1_scores,
            'cv_f1_mean': float(np.mean(fold_f1_scores)) if fold_f1_scores else None,
            'cv_f1_std': float(np.std(fold_f1_scores)) if fold_f1_scores else None,
            'method_type': 'few_shot_learning'
        }
        studies[method] = study
        
        print(f"\n{method_names[method]} å®Œæˆ:")
        print(f"  äº¤å‰éªŒè¯å‡†ç¡®ç‡: {study.best_value:.4f}")
        print(f"  è¯•éªŒæ¬¡æ•°: {len(study.trials)}")
        
    except Exception as e:
        print(f"\n{method_names[method]} å¤±è´¥: {str(e)}")
        print(f"  é”™è¯¯ç±»å‹: {type(e).__name__}")
        # è®°å½•å¤±è´¥çš„æ–¹æ³•
        results[method] = {
            'best_accuracy': 0.0,
            'best_params': {},
            'n_trials': 0,
            'cv_fold_accuracies': None,
            'cv_mean': 0.0,
            'cv_std': 0.0,
            'cv_fold_f1_scores': None,
            'cv_f1_mean': 0.0,
            'cv_f1_std': 0.0,
            'method_type': 'few_shot_learning',
            'error': str(e)
        }
        continue

# å†ä¼˜åŒ–ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
print("\n" + "="*80)
print("ç¬¬äºŒé˜¶æ®µï¼šä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•ä¼˜åŒ–")
print("="*80)

# åˆ›å»ºä¼ ç»Ÿæœºå™¨å­¦ä¹ è®­ç»ƒå™¨
traditional_trainer = TraditionalMLTrainer(train_features, train_labels, test_features, test_labels, train_df=train_data, processor=processor)

for method in traditional_ml_methods:
    print(f"\n{'='*60}")
    print(f"å¼€å§‹ä¼˜åŒ–: {method_names[method]}")
    print(f"{'='*60}")
    
    try:
        # æ‰§è¡Œä¼˜åŒ–
        study = traditional_trainer.optimize(method)
        best_params = study.best_params
        final_model = traditional_trainer.train_final_model(method, best_params)
        fold_scores = study.best_trial.user_attrs.get('fold_scores', None)
        fold_f1_scores = study.best_trial.user_attrs.get('fold_f1_scores', None)
        
        results[method] = {
            'best_accuracy': study.best_value,
            'best_params': study.best_params,
            'n_trials': len(study.trials),
            'cv_fold_accuracies': fold_scores,
            'cv_mean': float(np.mean(fold_scores)) if fold_scores else None,
            'cv_std': float(np.std(fold_scores)) if fold_scores else None,
            'cv_fold_f1_scores': fold_f1_scores,
            'cv_f1_mean': float(np.mean(fold_f1_scores)) if fold_f1_scores else None,
            'cv_f1_std': float(np.std(fold_f1_scores)) if fold_f1_scores else None,
            'method_type': 'traditional_ml'
        }
        studies[method] = study
        
        print(f"\n{method_names[method]} å®Œæˆ:")
        print(f"  äº¤å‰éªŒè¯å‡†ç¡®ç‡: {study.best_value:.4f}")
        print(f"  è¯•éªŒæ¬¡æ•°: {len(study.trials)}")
        
    except Exception as e:
        print(f"\n {method_names[method]} å¤±è´¥: {str(e)}")
        print(f"  é”™è¯¯ç±»å‹: {type(e).__name__}")
        # è®°å½•å¤±è´¥çš„æ–¹æ³•
        results[method] = {
            'best_accuracy': 0.0,
            'best_params': {},
            'n_trials': 0,
            'cv_fold_accuracies': None,
            'cv_mean': 0.0,
            'cv_std': 0.0,
            'cv_fold_f1_scores': None,
            'cv_f1_mean': 0.0,
            'cv_f1_std': 0.0,
            'method_type': 'traditional_ml',
            'error': str(e)
        }
        continue

# ä¿å­˜æ‰€æœ‰ç»“æœ
print("\n" + "="*80)
print("å®éªŒæ€»ç»“")
print("="*80)

# æŒ‰å‡†ç¡®ç‡æ’åºæ˜¾ç¤ºç»“æœ
sorted_methods = sorted(results.keys(), key=lambda x: results[x]['best_accuracy'], reverse=True)

print("\næ‰€æœ‰æ–¹æ³•æ€§èƒ½æ’å:")
for i, method in enumerate(sorted_methods, 1):
    result = results[method]
    method_display_name = method_names.get(method, str(method))  # ç¡®ä¿ä¸ä¸ºNone
    cv_std = result.get('cv_std', 0.0)
    if cv_std is None:
        cv_std = 0.0
        
    print(f"{i:2d}. {method_display_name:12s} - "
            f"å‡†ç¡®ç‡: {result['best_accuracy']:.4f} Â± {cv_std:.4f} "
            f"({result['method_type']})")

# æ–°å¢ï¼šæµ‹è¯•é›†è¯„ä¼°ä»¥æ£€æµ‹è¿‡æ‹Ÿåˆ
print("\n" + "="*80)
print("æµ‹è¯•é›†è¯„ä¼°")


test_results = {}

# è¯„ä¼°Few-Shot Learningæ–¹æ³•
for method in few_shot_methods:
    if method not in results or 'error' in results[method]:
        continue
        
    print(f"\nè¯„ä¼° {method_names[method]} åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½...")
    try:
        # ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒæ¨¡å‹
        trainer = UnifiedTrainer(method, train_features, train_labels, test_features, test_labels, train_data, processor)
        if method == 'metric_learning':
            models = trainer.train_metric_learning(results[method]['best_params'])
            probe_model = MetricLearningModel(
                input_size=trainer.input_size,
                hidden_size=results[method]['best_params']['hidden_size'],
                embedding_size=results[method]['best_params']['embedding_size'],
                num_classes=trainer.num_classes,
                dropout_rate=results[method]['best_params']['dropout_rate'],
                device=trainer.device
            )
            # æŠŠè®­ç»ƒå¥½çš„ trunk/embedder æƒé‡æ‹·å› probe_modelï¼ˆåˆ†ç±»å¤´æ˜¯å¦åŒæ­¥ä¸å½±å“çº¿æ€§æ¢é’ˆï¼‰
            # probe_model.backbone.trunk.load_state_dict(models['trunk'].state_dict())
            # probe_model.backbone.embedder.load_state_dict(models['embedder'].state_dict())
            trainer._load_metric_backbone_weights_(probe_model, models)
            sw_acc, sw_f1 = trainer.evaluate_few_shot_samplewise(probe_model, clf_type="logreg")
            results[method]['test_accuracy_sw'] = sw_acc
            results[method]['test_f1_sw'] = sw_f1
            results[method]['overfitting_gap_sw'] = results[method]['best_accuracy'] - sw_acc

            print(f"  (linear-probe)æµ‹è¯•é›†: Acc={sw_acc:.4f}, F1={sw_f1:.4f}")

        else:
            # MatchingNet / ProtoNet ä¿æŒä¸ä¸Šä¸€æ¡ç›¸åŒï¼šepisodic + sample-wiseï¼ˆçº¿æ€§æ¢é’ˆï¼‰
            episodic_model = trainer.train_few_shot(results[method]['best_params'])
            epi_acc, epi_f1 = trainer.evaluate_few_shot(episodic_model)
            sw_acc, sw_f1   = trainer.evaluate_few_shot_samplewise(episodic_model, clf_type="logreg")

            results[method]['test_accuracy_ep'] = epi_acc
            results[method]['test_f1_ep'] = epi_f1
            results[method]['test_accuracy_sw'] = sw_acc
            results[method]['test_f1_sw'] = sw_f1
            results[method]['overfitting_gap_sw'] = results[method]['best_accuracy'] - sw_acc

            print(f"  (episodic)    æµ‹è¯•é›†: Acc={epi_acc:.4f}, F1={epi_f1:.4f}")
            print(f"  (samplewise)  æµ‹è¯•é›†: Acc={sw_acc:.4f}, F1={sw_f1:.4f}")

    except Exception as e:
        print(f"  è¯„ä¼°å¤±è´¥: {e}")
        continue

# è¯„ä¼°ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
for method in traditional_ml_methods:
    if method not in results or 'error' in results[method]:
        continue
        
    print(f"\nè¯„ä¼° {method_names[method]} åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½...")
    try:
        # ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒæ¨¡å‹
        best_params = results[method]['best_params']
        final_model = traditional_trainer.train_final_model(method, best_params)
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
        test_predictions = final_model.predict(test_features)
        test_accuracy = accuracy_score(test_labels, test_predictions)
        test_f1 = f1_score(test_labels, test_predictions, average='weighted')
        
        results[method]['test_accuracy'] = test_accuracy
        results[method]['test_f1'] = test_f1
        cv_accuracy = results[method]['best_accuracy']
        cv_f1 = results[method]['cv_f1_mean']
        overfitting_gap = cv_accuracy - test_accuracy
        
        print(f"  äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_accuracy:.4f}")
        print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡:   {test_accuracy:.4f}")
        print(f"  äº¤å‰éªŒè¯F1:     {cv_f1:.4f}")
        print(f"  æµ‹è¯•é›†F1:       {test_f1:.4f}")
        print(f"  æ€§èƒ½å·®è·:       {overfitting_gap:.4f}")
        
            
    except Exception as e:
        print(f"  æµ‹è¯•é›†è¯„ä¼°å¤±è´¥: {str(e)}")
        results[method]['test_accuracy'] = 0.0
        results[method]['test_f1'] = 0.0



save_comparison_results(results, studies)